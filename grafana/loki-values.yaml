# helm install -f loki-values.yaml --namespace default loki grafana/loki-distributed --wait (--atomic)
# helm upgrade -f loki-values.yaml --namespace default loki grafana/loki-distributed --wait (--atomic)

# -- Overrides the chart's computed fullname
fullnameOverride: loki

compactor:
  # -- Specifies whether compactor should be enabled
  enabled: true

# Pod configuration for the index-gateway
indexGateway:
  enabled: true

loki:
  # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas
  schemaConfig:
    configs:
    # New TSDB schema below
    - from: "2024-04-15"
      index:
        period: 24h
        prefix: index_
      object_store: aws
      schema: v12
      store: tsdb

  # -- Check https://grafana.com/docs/loki/latest/configuration/#storage_config for more info on how to configure storages
  storageConfig:
    aws:
      http_config:
        response_header_timeout: 10s
      region: mac
      s3: http://minioadmin:minioadmin@host.docker.internal.:20090/loki
      s3forcepathstyle: true
    # not used only for reference as helmchart not ready for tsdb support yet
    boltdb_shipper:
      active_index_directory: /var/loki/index
      cache_location: /var/loki/cache
      cache_ttl: 168h
      shared_store: s3
    filesystem: null
    # New tsdb-shipper configuration
    tsdb_shipper:
      active_index_directory: /var/loki/tsdb-index
      cache_location: /var/loki/tsdb-cache
      index_gateway_client:
        # server_address taken from loki.congig.storage_config generated for boltdb_shipper
        server_address: dns:///loki-index-gateway:9095
      shared_store: s3

  # used for part of config not handled by template loki.config
  structuredConfig:
    compactor:
      shared_store: aws
      retention_enabled: true
      # otherwise pod ends in crashloop (mkdir : no such file or directory)
      working_directory: /var/loki/compactor
    limits_config:
      retention_period: 168h # 7days
    querier:
      # Each `querier` component process runs a number of parallel workers to process queries simultaneously.
      # You may want to adjust this up or down depending on your resource usage
      # (more available cpu and memory can tolerate higher values and vice versa),
      # but we find the most success running at around `16` with tsdb
      max_concurrent: 16
    query_scheduler:
      # the TSDB index dispatches many more, but each individually smaller, requests.
      # We increase the pending request queue sizes to compensate.
      max_outstanding_requests_per_tenant: 32768

# Pod configuration for query scheduler
queryScheduler:
  # docker desktop k8s - pods could be on same host
  affinity: |
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
            - key: kubernetes.io/hostname
              operator: In
              values:
                - docker-desktop
  enabled: true
  # -- Number of replicas for the query-scheduler.
  # It should be lower than `-querier.max-concurrent` to avoid generating back-pressure in queriers;
  # it's also recommended that this value evenly divides the latter
  replicas: 2
